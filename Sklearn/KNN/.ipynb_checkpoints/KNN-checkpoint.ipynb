{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors\n",
    "This notebook will start by covering what K-Nearest Neighbors (KNN) is, how it works, and how to use KNN in Python. Throughout this notebook we will also go over what pipelines are and how to use them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is K-Nearest Neighbors\n",
    "\n",
    "K-nearest neighbors is a model that uses the \"K\" most similar observations in order to make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "# Couldn't identify the source of this video. \n",
    "Video(\"images/KNN-Classification.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is roughly how K-Nearest Neighbors works:\n",
    "1. User specifies value for K. In this example above, we choose K=5 neighbors around black point.\n",
    "2. Search for the K observations in the data that are nearest to the measurements of an unknown sample\n",
    "    * Euclidian distance is often used as the distance metric\n",
    "3. Use the most popular target value from the K nearest neighbors as the predicted target value. In the example above, out of 5 nearest neighbors of black point, 2 are brown and 3 are green. Since we have a majority of green points around this black point we assign green label to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Advantages of KNN</b>\n",
    "\n",
    "Easier to understand and explain than other machine learning algorithms\n",
    "\n",
    "Can be used for classification or regression\n",
    "\n",
    "<b>Disadvantages of KNN</b>\n",
    "\n",
    "It must store all of the training data. \n",
    "\n",
    "Its prediction phase can be slow when n is large\n",
    "\n",
    "Typically worse performance than other supervised learning methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# For scaling data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "The Iris dataset is one of datasets scikit-learn comes with that do not require the downloading of any file from some external website. The code below loads the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrange Data into Features Matrix and Target Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstrational purposes, we are going take two features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:, ['sepal length (cm)', 'sepal width (cm)']]\n",
    "#X = df.loc[:, df.columns != 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.loc[:, 'target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state = 0,\n",
    "                                                    test_size = .2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN in `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 1:</b> Import the model you want to use\n",
    "\n",
    "In sklearn, all machine learning models are implemented as Python classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 2:</b> Make an instance of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 3:</b> Train the model on the data, storing the information learned from the data. Model is learning the relationship between features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 4:</b> Predict the labels of new data\n",
    "\n",
    "Uses the information the model learned during the model training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate classification accuracy\n",
    "score = knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap_light = ListedColormap(['orange', 'cyan', 'cornflowerblue'])\n",
    "cmap_bold = ListedColormap(['darkorange', 'c', 'darkblue'])\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X_train.loc[:, 'sepal length (cm)'].values.min() - 1, X_train.loc[:, 'sepal length (cm)'].values.max() + 1\n",
    "y_min, y_max = X_train.loc[:, 'sepal width (cm)'].values.min() - 1, X_train.loc[:, 'sepal width (cm)'].values.max() + 1\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure()\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X_train.loc[:, 'sepal length (cm)'].values,\n",
    "            X_train.loc[:, 'sepal width (cm)'].values,\n",
    "            c=y_train,\n",
    "            cmap=cmap_bold,\n",
    "            edgecolor='k',\n",
    "            s=20)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"3-Class classification (k = 5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning k\n",
    "When k is low, KNN is considered a low bias, high variance model. \n",
    "\n",
    "When k is high, KNN is considered a high bias, low variance model. \n",
    "\n",
    "In the video, as K is increased, the classification spaces' borders become more distinct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Source not clear for this video\n",
    "# Maybe machinelearningknowledge?\n",
    "Video(\"images/KNNlowtoHigh.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code that generated the images for the video video\n",
    "\"\"\"\n",
    "for num_neighbors in range(1, 51):\n",
    "\n",
    "    # Make an instance of the Model\n",
    "    knn = KNeighborsClassifier(n_neighbors=num_neighbors)\n",
    "\n",
    "    # Train the model on the data\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    cmap_light = ListedColormap(['orange', 'cyan', 'cornflowerblue'])\n",
    "    cmap_bold = ListedColormap(['darkorange', 'c', 'darkblue'])\n",
    "    h = .005  # step size in the mesh\n",
    "\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    x_min, x_max = X_train.loc[:, 'sepal length (cm)'].values.min() - 1, X_train.loc[:, 'sepal length (cm)'].values.max() + 1\n",
    "    y_min, y_max = X_train.loc[:, 'sepal width (cm)'].values.min() - 1, X_train.loc[:, 'sepal width (cm)'].values.max() + 1\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    \n",
    "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(figsize = (7,7))\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X_train.loc[:, 'sepal length (cm)'].values,\n",
    "                X_train.loc[:, 'sepal width (cm)'].values,\n",
    "                c=y_train,\n",
    "                cmap=cmap_bold,\n",
    "                edgecolor='k',\n",
    "                s=40)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(fontsize = 15)\n",
    "    plt.yticks(fontsize = 15)\n",
    "    plt.title(\"3-Class classification k = \" + str(num_neighbors), fontsize = 15)\n",
    "    plt.savefig('imagesanimation/' + 'initial' + str(num_neighbors).zfill(4) + '.png', dpi = 50)\n",
    "    plt.cla()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore\n",
    "#!ffmpeg -framerate 1 -i 'initial%04d.png' -c:v libx264 -r 30 -pix_fmt yuv420p initial_002.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits of Pipelines\n",
    "Pipelines are a simply way to keep your data processing and modeling code organized. Specifically a pipeline bundles preprocessing and modeling steps so you can use the whole bundle as if it were a single step.\n",
    "\n",
    "* Cleaner Code: You don’t need to keep track of your training data at each step of processing. Accounting for data at each step of processing can get messy. \n",
    "* Fewer Bugs: There are fewer opportunities to mis-apply a step or forget a pre-processing step\n",
    "* More options for model testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrange Data into Features Matrix and Target Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:, df.columns != 'target']\n",
    "y = df.loc[:, 'target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state = 0,\n",
    "                                                    test_size = .2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN in `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimension to 2 with PCA\n",
    "std_clf = make_pipeline(StandardScaler(),\n",
    "                        PCA(n_components=2, random_state=0),\n",
    "                        KNeighborsClassifier(n_neighbors=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_clf.fit(X_train, y_train)\n",
    "pred_test_std = std_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nPrediction accuracy for the standardized test dataset with PCA')\n",
    "print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test_std)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PCA from pipeline\n",
    "pca_std = std_clf.named_steps['pca']\n",
    "\n",
    "# Use PCA with scale on X_train data for visualization.\n",
    "scaler = std_clf.named_steps['standardscaler']\n",
    "X_train_std_transformed = pca_std.transform(scaler.transform(X_train))\n",
    "\n",
    "# visualize standardized  with PCA performed\n",
    "for l, c, m in zip(range(0, 3), ('blue', 'red', 'green'), ('^', 's', 'o')):\n",
    "    plt.scatter(X_train_std_transformed[y_train == l, 0],\n",
    "                X_train_std_transformed[y_train == l, 1],\n",
    "                color=c,\n",
    "                label='class %s' % l,\n",
    "                alpha=0.5,\n",
    "                marker=m\n",
    "                )\n",
    "\n",
    "plt.title('Standardized training dataset after PCA')\n",
    "plt.xlabel('1st principal component')\n",
    "plt.ylabel('2nd principal component')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
